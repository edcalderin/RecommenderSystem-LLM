# RecommenderSystem-LLM
Recommender System with Large Language Models (LLM)

## Arquitecture

This recommender system uses SentenceTransformer model to achieve embedding the
preprocessed prompts. 
In order to upgrade or deploy the system with new jobs data just replace the current dataframe with the new one containing new rows, then re-run the app for generating new job embeddings based on its prompt. This approach is literally the same as the user's dataset.

The ideal scenario would consist of storing those datasets into an external storage system such as an AWS S3 Bucket or GCP to avoid opening the project, so it would ease even more the project to be able to automatically fetch new data source on each deploy.


## How did I do it?
Firstly, I researched among different available LLM models but some of them are not free-to-use and need a good accelerated processing memory (CUDA GPU), having that said I ended up using SentenceTransformer which provided the necessary tools to create a stable recommender system, moreover it has a built-in function to calculate cosine similarity between two tensors which gave me more benefits. Since then, I was not exempt of long waits but it was the better option considering my computational limitation. In fact, I intended to use Transformers as these are more complete tools however I came across with allocation-memory errors by generating embedding for the jobs dataset.

Despite the fact it is not possible to evaluate results with classic metrics, I could notice a correlation between the user profile and the recommended jobs which take to conclude that SentenceTransformer carried out this task well but it could get better by creating a more descriptive prompt or including more features on it.

## API
The system has two available endpoints:
* `/api/v1/recommend/:id_user`: This one will allow you create a list of job recommendatios by id user
* `/api/v1/history/?id_user=:id_user`: To return all previous recommendations or by user id. You must keep in mind that this endopoint can return one or many recommendations sorted recently, this is useful when the jobs dataset is upgraded and need to inspect how the results evolved accordingly.

## Steps to reproduce locally

### With tox
 1. Create a new environment:

    `python3 -m venv venv`  
    `source venv/bin/activate`
 1. Run `pip install -u tox`. It is not a project dependency but required to execute commands locally. See link in References section for further information.
 2. Run `tox -e run_app`
 3. Go to `https://127.0.0.1/:8000` (Welcome endpoint)

### With Docker
 1. Rename `env.env` file to `.env`
 2. Run `docker-compose build`
 3. Run `docker-compose up`
 4. Go to `https://127.0.0.1:8000/` (Welcome endpoint)

In both cases, be patient at the first time if you are running on CPU since it will download the model and generate the embeddings.

## Directory layout
```
.
├── .github                  # CI/CD workflows
├── app                      # FastAPI directory
|   ├── config/              # Files to manage global configuration variables and settings
|   ├── controllers/         # Controllers for the application according to MVC (Model-View-Controller) pattern for separations of concerns, inkove service package and create a response for the Api.
|   ├── dao/                 # Data access layer: Contains the files to make CRUD operations directly to the database.
|   ├── routes/              # Routes for the application: Recommendations and History
|   ├── schemas/             # Schemas or models used throught the application, ie. Response models.
|   ├── services/            # Services to invoke dao functions and carry out business-logic.
|   ├── utils/               # Include helper and data preprocessing functions
|   ├── main.py              # App startpoint
├── data                     # Documentation files (alternatively `doc`)
|   ├── processed/           # Processed data generated by the application
|   ├── raw/                 # Raw data (by default)
├── notebooks/               # Notebooks used prior to development
├── requirements/            # Requirements for dev and prod
├── tests/                   # Tests
└── tox.ini                  # Configure commands to automate the applications
└── README.md
```

## ✉️ Contact
**LinkedIn:** https://www.linkedin.com/in/erick-calderin-5bb6963b/  
**e-mail:** edcm.erick@gmail.com

## Enjoyed this content?
Explore more of my work on [Medium](https://medium.com/@erickcalderin) 

I regularly share insights, tutorials, and reflections on tech, AI, and more. Your feedback and thoughts are always welcome!

## References
* SentenceTransformers (https://huggingface.co/sentence-transformers)
* Tox (https://tox.wiki/en/latest/)
